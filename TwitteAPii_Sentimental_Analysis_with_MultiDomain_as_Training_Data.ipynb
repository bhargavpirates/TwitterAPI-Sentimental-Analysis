{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitteAPii Sentimental Analysis with MultiDomain as Training Data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavpirates/TwitterAPI-Sentimental-Analysis/blob/master/TwitteAPii_Sentimental_Analysis_with_MultiDomain_as_Training_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WYBnJB5OWJJ",
        "colab_type": "text"
      },
      "source": [
        " ## Sentiment analysis on DomainSentiment Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jfcPGd0PoBC",
        "colab_type": "text"
      },
      "source": [
        "### import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWE12i5TLoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "#from gensim.models import Word2Vec\n",
        "#from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YznrqLxyPtIQ",
        "colab_type": "text"
      },
      "source": [
        "### working this problem on googleCollab So mounting Google DRive data into Collab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgfkCB2bTNZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRhGSsO-TO9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r  \"/content/drive/My Drive/domain_sentiment_data.tar/sorted_data_acl/books\" \"books\"\n",
        "!cp -r  \"/content/drive/My Drive/domain_sentiment_data.tar/sorted_data_acl/dvd\" \"dvd\"\n",
        "!cp -r  \"/content/drive/My Drive/domain_sentiment_data.tar/sorted_data_acl/electronics\" \"electronics\"\n",
        "!cp -r  \"/content/drive/My Drive/domain_sentiment_data.tar/sorted_data_acl/kitchen_&_housewares\" \"kitchen_&_housewares\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uRDYgFXP5wD",
        "colab_type": "text"
      },
      "source": [
        "## From XML Data extracting requried ReviewText Data and storing them in a .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7m8L8dTRtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_list=[\"dvd/\",\"books/\",\"electronics/\",\"kitchen_&_housewares/\"]\n",
        "for idx in range(4):\n",
        "\n",
        "    pos_file = data_list[idx] +\"positive.review\"\n",
        "    neg_file = data_list[idx] +\"negative.review\"\n",
        "   \n",
        "    with open(pos_file,'r',encoding='utf-8') as f:\n",
        "        pos = f.readlines()\n",
        "    with open(neg_file,'r',encoding='utf-8') as f:\n",
        "        neg = f.readlines()\n",
        "    \n",
        "    lst ,final_pos,final_neg=[],[],[]\n",
        "    \n",
        "    for i in range(len(pos)):\n",
        "        if(pos[i]==\"<review_text>\\n\"):lst.append(i)\n",
        "        elif(pos[i]==\"</review_text>\\n\"):lst.append(i)   \n",
        "        if(len(lst)==2):\n",
        "            a=pos[ lst[0]+1 : lst[1] ]\n",
        "            stng=\" \".join(a)\n",
        "            lst.clear()\n",
        "            final_pos.append(stng.replace(\"\\n\",\"\"))\n",
        "            \n",
        "    for i in range(len(neg)):\n",
        "        if(neg[i]==\"<review_text>\\n\"):lst.append(i)\n",
        "        elif(neg[i]==\"</review_text>\\n\"):lst.append(i)\n",
        "        if(len(lst)==2):\n",
        "            a=neg[ lst[0]+1 : lst[1] ]\n",
        "            stng=\" \".join(a)\n",
        "            lst.clear()\n",
        "            final_neg.append(stng.replace(\"\\n\",\"\"))\n",
        "     \n",
        "    \n",
        "    train , test = [], []  \n",
        "    train=final_pos[:int(len(final_pos)*0.8)] + final_neg[:int(len(final_neg)*0.8)]\n",
        "    test=final_pos[int(len(final_pos)*0.8):] + final_neg[int(len(final_neg)*0.8):]\n",
        "    \n",
        "    train_ylabel = [1 for i in range(int(len(final_pos)*0.8))] + [0 for i in range(int(len(final_pos)*0.8))]\n",
        "    \n",
        "    train_file = data_list[idx]+\"trainnew.txt\" \n",
        "    test_file = data_list[idx]+\"testnew.txt\"\n",
        "    \n",
        "    with open(train_file,'w',encoding='utf-8') as f:\n",
        "        for i in range(len(train)):\n",
        "            f.write(train[i])\n",
        "            f.write(\"\\n\")\n",
        "            \n",
        "    with open(test_file,'w',encoding='utf-8') as f:\n",
        "        for i in range(len(test)):\n",
        "            f.write(test[i])\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV2DMuoOQSmk",
        "colab_type": "text"
      },
      "source": [
        "### importing nltk library\n",
        "*  for stopwords\n",
        "* Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ-xnxpmSvAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq5punLYdWzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()    \n",
        "\n",
        "stopwords=set(stopwords.words('english'))        \n",
        "no_stopwords=[\"against\",\"aren't\",\"couldn't\",'didn',\"didn't\", \"doesn't\", \"don't\", 'few', \"hadn't\", \"hasn't\", \"haven't\",\"isn't\" ,'just',\"mightn't\",'more','most',\n",
        " \"mustn't\",\"no\",\"nor\",\"not\",\"needn't\",\"once\",\"out\",\"wasn't\",\"weren't\", \"won't\" , 'won', \"wouldn't\",'why','any','only','very']\n",
        "modified_stopwords=list(set(stopwords) - set(no_stopwords))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHoRiZ0xQgGw",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLZjiKGTU_Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "#reading file\n",
        "def preprocess(filename):\n",
        "    lst=[]\n",
        "    print(filename)\n",
        "    \n",
        "    for file in filename:\n",
        "      with open(file ,\"r\") as f:\n",
        "        for line in f.read().split('\\n'):\n",
        "          lst.append(line)\n",
        "          \n",
        "    #print(\"length of lst  :::: {} \".format(len(lst)))\n",
        "    lst = [ i.replace(\"aren't\",\"are not\").replace(\"couldn't\",\"could not\").replace(\"wasn't\",\"was not\") for i in lst ]\n",
        "    lst = [ i.replace(\"weren't\",\"were not\").replace(\"mustn't\",\"must not\").replace(\"won't\",\"not\").replace(\"wouldn't\",\"would not\") for i in lst ]\n",
        "    lst = [re.sub(r'[^a-zA-Z ]',r'' , (\" \".join([lemmatizer.lemmatize(i.lower()) for i in line.split()  if i not in modified_stopwords])) ) for line in lst ]\n",
        "    lst = [line for line in lst if line!='']\n",
        "    #lst = lst[:-1]\n",
        "    \n",
        "    return lst\n",
        "    \n",
        "\n",
        "x_train=preprocess([\"dvd/trainnew.txt\",\"books/trainnew.txt\",\"electronics/trainnew.txt\",\"kitchen_&_housewares/trainnew.txt\"])\n",
        "x_test=preprocess([\"dvd/testnew.txt\",\"books/testnew.txt\",\"electronics/testnew.txt\",\"kitchen_&_housewares/testnew.txt\"])\n",
        "\n",
        "y_train=[]\n",
        "y_train= [1 if(i<800) else 0 for i in range(1600)]*4\n",
        "y_test= [1 if(i<200) else 0 for i in range(400)]*4\n",
        "\n",
        "print(\"length of x_train ::: {}\".format(len(x_train)))\n",
        "print(\"length of x_test ::: {}\".format(len(x_test)))\n",
        "print(\"length of y_train ::: {}\".format(len(y_train)))\n",
        "print(\"length of y_test ::: {}\".format(len(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYvmenNGQogW",
        "colab_type": "text"
      },
      "source": [
        "## Coverting Text Data into Vector Using TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07e5XDdLiG_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_vect = TfidfVectorizer(ngram_range=(1,3), min_df=10, max_df=0.95,stop_words='english',max_features=50000 )\n",
        "tf_idf_vect.fit(x_train)\n",
        "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\n",
        "print('='*50)\n",
        "\n",
        "X_train_tfidf= tf_idf_vect.transform(x_train)\n",
        "print(\"the type of count vectorizer \",type(X_train_tfidf))\n",
        "print(\"the shape of out text TFIDF vectorizer \",X_train_tfidf.get_shape())\n",
        "print(\"the number of unique words including both unigrams and bigrams \", X_train_tfidf.get_shape()[1])\n",
        "\n",
        "X_test_tfidf = tf_idf_vect.transform(x_test)\n",
        "print(\"the type of count vectorizer \",type(X_test_tfidf))\n",
        "print(\"the shape of out text TFIDF vectorizer \",X_test_tfidf.get_shape())\n",
        "print(\"the number of unique words \", X_test_tfidf.get_shape()[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNfva4utQ0UH",
        "colab_type": "text"
      },
      "source": [
        "### Applying Standard Scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsBAmdDWjbMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "scaler.fit(X_train_tfidf)\n",
        "\n",
        "X_train_tfidf=scaler.transform(X_train_tfidf)\n",
        "X_test_tfidf=scaler.transform(X_test_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyYv805eQ5An",
        "colab_type": "text"
      },
      "source": [
        "## MODEL 1 :: Applying LogisticRegression using penalty ='L2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1tdTJTvRS20",
        "colab_type": "text"
      },
      "source": [
        "### Finding Best HyperParaneter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72QJaGoHjqRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#algoname=LogisticRegression\n",
        "#param_grid={'C':[1000,100,10,1,0.1,0.001,0.0001,0.00001]}\n",
        "def algo(algoname,param_grid,plot=True):\n",
        "  clf=algoname()\n",
        "\n",
        "  gcv=GridSearchCV(clf,param_grid,cv=5,scoring='roc_auc')\n",
        "  gcv.fit(X_train_tfidf,y_train)\n",
        "  print(gcv.best_params_)\n",
        "  print(gcv.best_score_)\n",
        "  \n",
        "  val=list(param_grid.keys())\n",
        "  print(val)\n",
        "  hyper_parameters=gcv.get_params()['param_grid'][val[0]]\n",
        "  test_scores=gcv.cv_results_['mean_test_score'].tolist()\n",
        "\n",
        "  log_hyper_parameters=[math.log(i) for i in hyper_parameters]\n",
        "  if(plot):\n",
        "    plt.plot( log_hyper_parameters ,test_scores , color='green', linestyle='dashed', linewidth = 3,marker='o', markerfacecolor='blue', markersize=12,  label='Test plot')\n",
        "    plt.xlabel(\"hyper_parameters (C)\")\n",
        "    plt.ylabel(\"Model performance\")\n",
        "    plt.legend()\n",
        "  \n",
        "  print(list(gcv.best_params_.values()))\n",
        "  \n",
        "  return list(gcv.best_params_.values())[0]\n",
        "  #print(gcv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64uCtaQHootT",
        "colab_type": "text"
      },
      "source": [
        "#### https://stackoverflow.com/questions/22851316/what-is-the-inverse-of-regularization-strength-in-logistic-regression-how-shoul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htxQ7GZrsxuN",
        "colab_type": "text"
      },
      "source": [
        "### Finding Metrics values , ploting ROC_AUC Curve and Confuion MAtrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9A63kKnjy40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def metric(algoname,best_hyperparameter):\n",
        "  #clf1=algoname(C=best_hyperparameter)\n",
        "  #print(param)\n",
        "  if(algoname==LogisticRegression):\n",
        "    clf1=algoname(C=best_hyperparameter)\n",
        "  else:\n",
        "    clf1=algoname(best_hyperparameter)\n",
        "  clf1.fit(X_train_tfidf,y_train)\n",
        "  pred_train=clf1.predict(X_train_tfidf)\n",
        "  pred=clf1.predict(X_test_tfidf)\n",
        "\n",
        "  print(pred)\n",
        "  fpr_train,tpr_train,thresholds_train=roc_curve(y_train,pred_train)\n",
        "  print(\"AUC Score for train data  :\",metrics.auc(fpr_train,tpr_train))\n",
        "  fpr,tpr,thresholds=roc_curve(y_test,pred)\n",
        "  print(\"AUC Score for test data   :\",metrics.auc(fpr,tpr))\n",
        "  print(\"Accuracy Score            : \",accuracy_score(y_test,pred)*100)\n",
        "  print(\"Precision Score           : \",precision_score(y_test,pred)*100)\n",
        "  print(\"Recall Score              : \",recall_score(y_test,pred)*100)\n",
        "  print(\"F1 Score                  : \",f1_score(y_test,pred)*100)\n",
        "\n",
        "  confusionmatrix_DF=pd.DataFrame(confusion_matrix(y_test,pred),columns=['0','1'],index=['0','1'])\n",
        "  sns.heatmap(confusionmatrix_DF,annot=True,fmt='g',cmap='viridis')\n",
        "  plt.title(\"Confusion matrix \")\n",
        "  plt.show()\n",
        "  \n",
        "  return clf1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJfLnkMxIu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "algoname=LogisticRegression\n",
        "#algoname=LogisticRegression\n",
        "param_grid={'C':[1000,100,10,1,0.1,0.001,0.0001,0.00001]}\n",
        "best_hyperparameter= algo(algoname,param_grid,False)\n",
        "print(best_hyperparameter)\n",
        "clf1 = metric(algoname,best_hyperparameter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vje9MVfWRzOZ",
        "colab_type": "text"
      },
      "source": [
        "## Important Features on TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_CWagNdlAzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names=tf_idf_vect.get_feature_names()\n",
        "coefs=sorted(zip(clf1.coef_[0],feature_names))\n",
        "\n",
        "top20Negative=coefs[:5]\n",
        "top20Postive=coefs[::-1][:5]\n",
        "\n",
        "res_neg=pd.DataFrame(top20Negative,columns=['values','NegativeFeatures'])\n",
        "res_pos=pd.DataFrame(top20Postive,columns=['values','PostiveFeatures'])\n",
        "pd.concat([res_neg,res_pos],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qkBBEvJtADM",
        "colab_type": "text"
      },
      "source": [
        "## Predicting unlabeled Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxX_dvolUiO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_list=[\"dvd/\",\"electronics/\",\"kitchen_&_housewares/\"]\n",
        "final_unlabeled=[]\n",
        "for idx in range(3):\n",
        "  unlabeled_file = data_list[idx] +\"unlabeled.review\"\n",
        "  with open(unlabeled_file,'r',encoding='utf-8') as f:\n",
        "    unlabeled = f.readlines()\n",
        "  lst = []\n",
        "  for i in range(len(unlabeled)):\n",
        "    if(unlabeled[i]==\"<review_text>\\n\"):lst.append(i)\n",
        "    elif(unlabeled[i]==\"</review_text>\\n\"):lst.append(i)   \n",
        "    if(len(lst)==2):\n",
        "      a=unlabeled[ lst[0]+1 : lst[1] ]\n",
        "      lst.clear()\n",
        "      final_unlabeled.append(\" \".join(a).replace(\"\\n\",\"\"))\n",
        "  print(\"total number of rows in final_unlabeled file :::: {}\".format(len(final_unlabeled)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H64Qg-z7VZ75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_unlabeled = [ i.replace(\"aren't\",\"are not\").replace(\"couldn't\",\"could not\").replace(\"wasn't\",\"was not\") for i in final_unlabeled ]\n",
        "final_unlabeled = [ i.replace(\"weren't\",\"were not\").replace(\"mustn't\",\"must not\").replace(\"won't\",\"not\").replace(\"wouldn't\",\"would not\") for i in final_unlabeled ]\n",
        "final_unlabeled = [re.sub(r'[^a-zA-Z ]',r'' , (\" \".join([lemmatizer.lemmatize(i.lower()) for i in line.split()  if i not in modified_stopwords])) ) for line in final_unlabeled ]\n",
        "final_unlabeled = [line for line in final_unlabeled if line!='']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9RU_4lbbn_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unlabeled_tfidf          =  tf_idf_vect.transform(final_unlabeled)\n",
        "scalar_unlabeled_tfidf   =  scaler.transform(unlabeled_tfidf)\n",
        "unlabel_predicted        =  clf1.predict(scalar_unlabeled_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5M79rnuzsMY",
        "colab_type": "text"
      },
      "source": [
        " ### Model2  :: MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2xnvf7WzzTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  sklearn.naive_bayes import MultinomialNB\n",
        "algoname=MultinomialNB\n",
        "#alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
        "param_grid={'alpha':[1000,100,10,1,0.1,0.001,0.0001,0.00001]}\n",
        "best_hyperparameter=algo(algoname,param_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fL6XgMkYeXQo",
        "colab": {}
      },
      "source": [
        "#algoname=LogisticRegression\n",
        "#best_hyperparameter= algo(algoname,param_grid,False)\n",
        "#clf1 = metric(algoname,best_hyperparameter)\n",
        "\n",
        "best_hyperparameter= algo(algoname,param_grid,False)\n",
        "print(best_hyperparameter)\n",
        "clf1 = metric(algoname,best_hyperparameter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k246v7d5domR",
        "colab_type": "text"
      },
      "source": [
        "### Remaning Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqlaJnwSDsu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install xgboost\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCS3JVNDtDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "#dtrain = xgb.DMatrix(X_train_xg.values, label=y_train_xg.values)\n",
        "#dtest = xgb.DMatrix(X_test_xg.values, label=y_test_xg.values)\n",
        "param_test1 = {\n",
        "'max_depth': [5,10,12,16,20],\n",
        "'min_child_weight':[2,4,6,8],\n",
        "'reg_alpha':[0.00001,0.001,0.01,1]\n",
        "}\n",
        "\n",
        "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =\n",
        "0.1, n_estimators=140, max_depth=5,\n",
        "min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
        "objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
        "param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
        "gsearch1.fit(X_train_tfidf,y_train)\n",
        "#gsearch1.cv_results_\n",
        "gsearch1.best_params_, gsearch1.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq527kDxDycB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics.classification import accuracy_score, log_loss\n",
        "clf1=xgb.XGBClassifier(min_child_weight=2,max_depth=16 , reg_alpha=0.01)\n",
        "clf1.fit(X_train_tfidf,y_train)\n",
        "pred_train=clf1.predict(X_train_tfidf)\n",
        "pred=clf1.predict(X_test_tfidf)\n",
        "print(pred)\n",
        "\n",
        "\n",
        "print(\"Accuracy Score : \",accuracy_score(y_test,pred)*100)\n",
        "print(\"Precision Score : \",precision_score(y_test,pred)*100)\n",
        "print(\"Recall Score : \",recall_score(y_test,pred)*100)\n",
        "print(\"F1 Score : \",f1_score(y_test,pred)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuiDGo11D3eH",
        "colab_type": "text"
      },
      "source": [
        "# Example:: Twitter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDlWQ1f3D60f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m pip install tweepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsLmQkBuD8pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "import re\n",
        "\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "consumer_api_key = 'ED7DTkkIg7bkAsrUt3Lj5kSM6'\n",
        "consumer_api_secret = 'TJA4rA7nRVhO5dmGZnQPhdegPyqgSuGt4exxyCcCnsqJXi6WFT' \n",
        "access_token = '516506229-qdHN2J6EACuFvuyAXisB0Pe4zeJykOzufzieesxu'\n",
        "access_token_secret ='q8eig46jC4giFe6AFn8VnqkM32adW9uccLxVROpzbp2Ap'\n",
        "\n",
        "\n",
        "authorizer = OAuthHandler(consumer_api_key, consumer_api_secret)\n",
        "authorizer.set_access_token(access_token, access_token_secret)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXgFNJdCD-r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#api = tweepy.API(authorizer ,timeout=15)\n",
        "api = tweepy.API(authorizer, wait_on_rate_limit=True)\n",
        "\n",
        "all_tweets = []\n",
        "\n",
        "search_query = 'IsroPerGarvHai'\n",
        "\n",
        "for tweet_object in tweepy.Cursor(api.search,q=search_query+\" -filter:retweets\",lang='en',result_type='recent').items():\n",
        "    all_tweets.append(tweet_object.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ZJRQf_EiuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_tweets = [\"\".join(re.sub(r'https?:\\/\\/.*[\\r\\n]*','', i )) for i in all_tweets]\n",
        "all_tweets  = [\"\".join(re.sub(r'[^a-zA-Z0-9 ]','',i)) for i in all_tweets]\n",
        "tweet_scrap = [ i.replace(\"aren't\",\"are not\").replace(\"couldn't\",\"could not\").replace(\"wasn't\",\"was not\") for i in all_tweets ]\n",
        "tweet_scrap = [ i.replace(\"weren't\",\"were not\").replace(\"mustn't\",\"must not\").replace(\"won't\",\"not\").replace(\"wouldn't\",\"would not\") for i in all_tweets ]\n",
        "tweet_scrap = [re.sub(r'[^a-zA-Z ]',r'' , (\" \".join([lemmatizer.lemmatize(i.lower()) for i in line.split()  if i not in modified_stopwords])) ) for line in all_tweets ]\n",
        "tweet_data = [line for line in tweet_scrap if line!='']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vND7qP8feE6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_tfidf     =  tf_idf_vect.transform(tweet_data)\n",
        "scalar_tweet_tfidf     =  scaler.transform(tweet_tfidf)\n",
        "final_tweet_tfidf  =  clf1.predict(scalar_tweet_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI_bygM0eajx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_tweet_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQQABzkYeg3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_tweet_data = np.array(tweet_data)\n",
        "pd.DataFrame({\"Tweet_Data\" : np_tweet_data,\"label\" :final_tweet_tfidf})"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}